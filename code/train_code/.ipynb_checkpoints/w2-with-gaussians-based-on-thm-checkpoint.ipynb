{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# sys.path.append(\"./code/train_code/\")\n",
    "\n",
    "import config as config\n",
    "import metrics as metrics\n",
    "from config_populate import data_settings\n",
    "import exp_select as exp_select\n",
    "\n",
    "import importlib\n",
    "\n",
    "import gaussian_utils\n",
    "\n",
    "from trainer import MultiSourceTrainer\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from evaluate_model import get_target_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_config(dataset, task, exp_id):\n",
    "    config.dataset_name = dataset\n",
    "    config.data_key = task\n",
    "    \n",
    "    config.settings['dataset_dir']                 = os.path.join('data', 'pretrained-features', config.dataset_name)\n",
    "    \n",
    "    config.settings['C']                           = data_settings[config.dataset_name][config.data_key]['C']\n",
    "    config.settings['C_dash']                      = data_settings[config.dataset_name][config.data_key]['C_dash']\n",
    "    config.settings['num_C_dash']                  = data_settings[config.dataset_name][config.data_key]['num_C_dash']\n",
    "\n",
    "    config.settings['num_C']                       = data_settings[config.dataset_name][config.data_key]['num_C']\n",
    "    config.settings['src_datasets']                = data_settings[config.dataset_name][config.data_key]['src_datasets']\n",
    "    config.settings['trgt_datasets']               = data_settings[config.dataset_name][config.data_key]['trgt_datasets']\n",
    "    \n",
    "    config.settings['log_interval']                = config.settings['expt_dict'][config.dataset_name][config.data_key]['val_after']\n",
    "    config.settings['start_iter']                  = 0\n",
    "    config.settings['max_iter']                    = config.settings['expt_dict'][config.dataset_name][config.data_key]['max_iter']\n",
    "    config.settings['enough_iter']                 = config.settings['expt_dict'][config.dataset_name][config.data_key]['enough_iter']\n",
    "    config.settings['val_after']                   = config.settings['expt_dict'][config.dataset_name][config.data_key]['val_after']\n",
    "    config.settings['batch_size']                  = config.settings['expt_dict'][config.dataset_name][config.data_key]['batch_size']\n",
    "    config.settings['adapt_batch_size']            = config.settings['expt_dict'][config.dataset_name][config.data_key]['adapt_batch_size']\n",
    "    config.settings['val_batch_size_factor']       = config.settings['expt_dict'][config.dataset_name][config.data_key]['val_batch_size_factor']\n",
    "    \n",
    "    exp_select.exp_id = exp_id\n",
    "    config.settings['exp_name']                    = config.gen_exp_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the weights w minimizing the generalizability objective\n",
    "def learn_w_w2(it_thresh='max_iter', num_steps=100):\n",
    "    w2_dist = np.zeros(len(config.settings['src_datasets']))\n",
    "    \n",
    "    # The approximation is better if the batch size is larger. Let's see how large we can make it\n",
    "    multiplier = 5\n",
    "    for src_domain_idx in range(len(config.settings['src_datasets'])):\n",
    "        dom = config.settings['src_datasets'][src_domain_idx]\n",
    "        trainer = MultiSourceTrainer(src_domain_idx)\n",
    "        trainer.load_model_weights(it_thresh=it_thresh)\n",
    "        trainer.set_mode(trainer.settings['mode']['val'])\n",
    "        initial_batch_size = trainer.adapt_batch_size\n",
    "        \n",
    "        for curr_multiplier in range(1, 6):\n",
    "            print(\"Loading trainer for source domain {}\".format(config.settings['src_datasets'][src_domain_idx]))\n",
    "\n",
    "            # align the batch sizes\n",
    "            trainer.adapt_batch_size = initial_batch_size * curr_multiplier\n",
    "            trainer.batch_size = initial_batch_size * curr_multiplier\n",
    "\n",
    "            if trainer.batch_size > 600:\n",
    "                multiplier = min(multiplier, curr_multiplier - 1)\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                trainer.initialize_src_train_dataloader()\n",
    "                _,X_src,_,_ = trainer.source_dl_iter_train_list.next()\n",
    "                X_src = Variable(X_src).to(trainer.settings['device']).float()\n",
    "\n",
    "                trainer.initialize_target_adapt_dataloader()\n",
    "                _,X_tar,_,_ = trainer.adapt_target_dl_iter_train_list.next()\n",
    "                X_tar = Variable(X_tar).to(trainer.settings['device']).float()\n",
    "            except StopIteration:\n",
    "                print(\"Multiplier for {} is {}\".format(dom, curr_multiplier - 1))\n",
    "                multiplier = min(multiplier, curr_multiplier - 1)\n",
    "                break\n",
    "                \n",
    "    print('multiplier = {}'.format(multiplier))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    for src_domain_idx in range(len(config.settings['src_datasets'])):\n",
    "        print(\"Loading trainer for source domain {}\".format(config.settings['src_datasets'][src_domain_idx]))\n",
    "\n",
    "        dom = config.settings['src_datasets'][src_domain_idx]\n",
    "        trainer = MultiSourceTrainer(src_domain_idx)\n",
    "        \n",
    "        # learn the gaussians\n",
    "        trainer.load_model_weights(it_thresh='enough_iter')\n",
    "        gaussian_utils.learn_gaussians(trainer)\n",
    "        n_samples = np.ones(trainer.N_CLASSES, dtype=int) * trainer.settings[\"gaussian_samples_per_class\"] * 100\n",
    "        trainer.gaussian_z, trainer.gaussian_y = gaussian_utils.sample_from_gaussians(trainer.means, trainer.covs, n_samples)\n",
    "        \n",
    "        # If there are high confidence pseudo-labels for most of the samples, use those for adaptation\n",
    "#         trainer.load_model_weights(it_thresh='enough_iter')\n",
    "#         trainer.pseudo_target_dist, conf, _ = trainer.get_target_pseudo_distribution()\n",
    "#         if np.sum(np.max(conf, axis=-1) > trainer.settings['confidence_thresh']) / conf.shape[0] < trainer.settings['confidence_ratio']:\n",
    "        trainer.pseudo_target_dist = np.ones(trainer.N_CLASSES, dtype=int)\n",
    "            \n",
    "        print(\"Dist=\")\n",
    "        print(trainer.pseudo_target_dist)\n",
    "        \n",
    "        trainer.load_model_weights(it_thresh=it_thresh)\n",
    "        trainer.set_mode(trainer.settings['mode']['val'])\n",
    "        \n",
    "        trainer.adapt_batch_size *= multiplier\n",
    "        trainer.batch_size = trainer.adapt_batch_size    # align the batch sizes\n",
    "\n",
    "        trainer.initialize_src_train_dataloader()\n",
    "        trainer.initialize_target_adapt_dataloader()\n",
    "        \n",
    "        # Get the mean W2 distance between encodings of the current domain and the target domain\n",
    "        # Get a source sample\n",
    "        w2_dist[src_domain_idx] = 0\n",
    "        for step in range(num_steps):\n",
    "            # Get a target sample\n",
    "            try:\n",
    "                _,X_src,_,_ = trainer.source_dl_iter_train_list.next()\n",
    "                X_src = Variable(X_src).to(trainer.settings['device']).float()\n",
    "            except StopIteration:\n",
    "                trainer.initialize_src_train_dataloader()\n",
    "                _,X_src,_,_ = trainer.source_dl_iter_train_list.next()\n",
    "                X_src = Variable(X_src).to(trainer.settings['device']).float()\n",
    "\n",
    "            # Get a target sample\n",
    "            try:\n",
    "                _,X_tar,_,_ = trainer.adapt_target_dl_iter_train_list.next()\n",
    "                X_tar = Variable(X_tar).to(trainer.settings['device']).float()\n",
    "            except StopIteration:\n",
    "                trainer.initialize_target_adapt_dataloader()\n",
    "                _,X_tar,_,_ = trainer.adapt_target_dl_iter_train_list.next()\n",
    "                X_tar = Variable(X_tar).to(trainer.settings['device']).float()\n",
    "                \n",
    "            # Compute the number of gaussian samples to be used for the current batch\n",
    "            normalized_dist = trainer.pseudo_target_dist / np.sum(trainer.pseudo_target_dist)\n",
    "            num_samples = np.array(normalized_dist * trainer.adapt_batch_size, dtype=int)\n",
    "            while trainer.adapt_batch_size > np.sum(num_samples):\n",
    "                idx = np.random.choice(range(trainer.N_CLASSES), p = normalized_dist)\n",
    "                num_samples[idx] += 1\n",
    "\n",
    "            # Get gaussian samples for the current batch\n",
    "            gz = []\n",
    "            gy = []\n",
    "            for c in range(trainer.N_CLASSES):\n",
    "                ind = np.where(trainer.gaussian_y == c)[0]\n",
    "                ind = ind[np.random.choice(range(len(ind)), num_samples[c], replace=False)]\n",
    "                gz.append(trainer.gaussian_z[ind])\n",
    "                gy.append(trainer.gaussian_y[ind])\n",
    "            gz = np.vstack(gz)\n",
    "            gy = np.concatenate(gy)\n",
    "\n",
    "            gz = torch.as_tensor(gz).to(trainer.settings['device']).float()\n",
    "            gy = torch.as_tensor(gy).to(trainer.settings['device']).long()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                f_src = trainer.network.model['global']['Fs'](X_src)\n",
    "                f_tar = trainer.network.model['global']['Fs'](X_tar)\n",
    "                \n",
    "                d1 = metrics.sliced_wasserstein_distance(f_src, gz, trainer.settings['num_projections'], 2, trainer.settings['device']).item()\n",
    "                d2 = metrics.sliced_wasserstein_distance(f_tar, gz, trainer.settings['num_projections'], 2, trainer.settings['device']).item()\n",
    "                w2_dist[src_domain_idx] += d1 + d2\n",
    "    \n",
    "\n",
    "            if step % (num_steps // 10) == 0:\n",
    "                print(step, w2_dist, dom)\n",
    "                \n",
    "    return w2_dist\n",
    "        \n",
    "#     w = 1 / w2_dist\n",
    "#     w = w / np.sum(w)\n",
    "        \n",
    "#     return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "office-31\n",
      "AD_W\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "multiplier = 5\n",
      "\n",
      "\n",
      "\n",
      "Loading trainer for source domain amazon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amazon: 100%|██████████| 176/176 [00:00<00:00, 387.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist=\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0 [0.05901814 0.        ] amazon\n",
      "10 [0.64640579 0.        ] amazon\n",
      "20 [1.25674414 0.        ] amazon\n",
      "30 [1.90525379 0.        ] amazon\n",
      "40 [2.53242396 0.        ] amazon\n",
      "50 [3.14613749 0.        ] amazon\n",
      "60 [3.7923124 0.       ] amazon\n",
      "70 [4.42834812 0.        ] amazon\n",
      "80 [5.07870832 0.        ] amazon\n",
      "90 [5.68782908 0.        ] amazon\n",
      "Loading trainer for source domain dslr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dslr: 100%|██████████| 31/31 [00:00<00:00, 183.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist=\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0 [6.23233495 0.05392897] dslr\n",
      "10 [6.23233495 0.59802025] dslr\n",
      "20 [6.23233495 1.09320621] dslr\n",
      "30 [6.23233495 1.64965891] dslr\n",
      "40 [6.23233495 2.17245644] dslr\n",
      "50 [6.23233495 2.74557499] dslr\n",
      "60 [6.23233495 3.27255114] dslr\n",
      "70 [6.23233495 3.82315779] dslr\n",
      "80 [6.23233495 4.33155094] dslr\n",
      "90 [6.23233495 4.9015769 ] dslr\n",
      "Loading trainer for source domain amazon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "webcam: 100%|██████████| 3/3 [00:00<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer for source domain dslr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "webcam: 100%|██████████| 3/3 [00:00<00:00, 23.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer for source domain amazon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "webcam: 100%|██████████| 3/3 [00:00<00:00, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainer for source domain dslr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "webcam: 100%|██████████| 3/3 [00:00<00:00, 24.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('office-31', 'AD_W', 0, 0.9382871536523929, array([0.46475592, 0.53524408]))\n",
      "('office-31', 'AD_W', 0, 0.9382871536523929, array([0.46475592, 0.53524408]))\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain amazon\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "Loading trainer for source domain dslr\n",
      "multiplier = 5\n",
      "\n",
      "\n",
      "\n",
      "Loading trainer for source domain amazon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amazon: 100%|██████████| 176/176 [00:00<00:00, 342.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist=\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0 [0.06081343 0.        ] amazon\n",
      "10 [0.62572178 0.        ] amazon\n",
      "20 [1.17881612 0.        ] amazon\n",
      "30 [1.71961884 0.        ] amazon\n",
      "40 [2.27033443 0.        ] amazon\n",
      "50 [2.83790485 0.        ] amazon\n",
      "60 [3.40866809 0.        ] amazon\n",
      "70 [3.95061831 0.        ] amazon\n",
      "80 [4.50925705 0.        ] amazon\n",
      "90 [5.07676887 0.        ] amazon\n",
      "Loading trainer for source domain dslr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dslr: 100%|██████████| 31/31 [00:00<00:00, 191.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dist=\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0 [5.59201054 0.05270137] dslr\n",
      "10 [5.59201054 0.53290462] dslr\n",
      "20 [5.59201054 1.03393811] dslr\n",
      "30 [5.59201054 1.53718082] dslr\n",
      "40 [5.59201054 2.05301042] dslr\n",
      "50 [5.59201054 2.54216597] dslr\n"
     ]
    }
   ],
   "source": [
    "res_store = []\n",
    "res_store_2 = []\n",
    "\n",
    "for dataset in ['office-31', 'image-clef', 'office-caltech', 'office-home']:\n",
    "    tasks = config.settings['expt_dict'][dataset].keys()\n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    \n",
    "    for task in tasks:\n",
    "        print(task)\n",
    "\n",
    "        for exp in range(5):\n",
    "            change_config(dataset, task, exp)\n",
    "            \n",
    "            w = learn_w_w2()\n",
    "            w = w / np.sum(w)\n",
    "            \n",
    "            w_hat_w2 = 1 / np.copy(w)\n",
    "            w_hat_w2 = w_hat_w2 / np.sum(w_hat_w2)\n",
    "            res_store.append((dataset, task, exp, get_target_accuracy(w_hat_w2, 'max_iter')[0], np.copy(w_hat_w2)))\n",
    "            \n",
    "            w_hat_w2 = 1 - np.copy(w)\n",
    "            w_hat_w2 = w_hat_w2 / np.sum(w_hat_w2)\n",
    "            res_store_2.append((dataset, task, exp, get_target_accuracy(w_hat_w2, 'max_iter')[0], np.copy(w_hat_w2)))\n",
    "            \n",
    "            print(res_store[-1])\n",
    "            print(res_store_2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['office-31', 'image-clef', 'office-caltech', 'office-home']:\n",
    "    tasks = config.settings['expt_dict'][dataset].keys()\n",
    "    \n",
    "    dataset_avg = []\n",
    "    \n",
    "    for t in tasks:\n",
    "#         print(dataset, t)\n",
    "        avg = []\n",
    "        for r in res_store:\n",
    "            if r[0] == dataset and r[1] == t:\n",
    "                dataset_avg.append(r[3] * 100)\n",
    "                avg.append(r[3] * 100)\n",
    "#         avg = np.asarray(avg)\n",
    "        \n",
    "        print(dataset, t, np.mean(avg).round(1), np.var(avg).round(2))\n",
    "    print(dataset, np.mean(dataset_avg).round(1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for r in res_store:\n",
    "# #     print(r)\n",
    "    \n",
    "# # Writing to file\n",
    "# with open(\"../../run_benchmarks/updated_w2_scores.txt\", \"w\") as save_file:\n",
    "#     for dataset in ['office-31', 'image-clef', 'office-caltech', 'office-home']:\n",
    "#         tasks = config.settings['expt_dict'][dataset].keys()\n",
    "\n",
    "#         dataset_avg = []\n",
    "\n",
    "#         for t in tasks:\n",
    "#     #         print(dataset, t)\n",
    "#             avg = []\n",
    "#             for r in res_store:\n",
    "#                 if r[0] == dataset and r[1] == t:\n",
    "#                     dataset_avg.append(r[3] * 100)\n",
    "#                     avg.append(r[3] * 100)\n",
    "#     #         avg = np.asarray(avg)\n",
    "\n",
    "#             save_file.write(\"{} {} {} {}\\n\".format(dataset, t, np.mean(avg).round(1), np.var(avg).round(2)))\n",
    "#         save_file.write(\"{} {}\\n\".format(dataset, np.mean(dataset_avg).round(1)))\n",
    "#         save_file.write(\"\\n\")\n",
    "    \n",
    "#     # Writing data to a file\n",
    "#     for r in res_store:\n",
    "#         save_file.write(str(r) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
